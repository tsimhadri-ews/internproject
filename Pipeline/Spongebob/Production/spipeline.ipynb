{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bb3c9fd-0376-4130-a6ec-153459f29bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Using cached streamlit-1.37.0-py2.py3-none-any.whl (8.7 MB)\n",
      "Collecting protobuf<6,>=3.20\n",
      "  Downloading protobuf-5.27.3-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m309.3/309.3 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=7.0\n",
      "  Using cached pyarrow-17.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.9/site-packages (from streamlit) (21.3)\n",
      "Collecting blinker<2,>=1.0.0\n",
      "  Using cached blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.9/site-packages (from streamlit) (8.1.3)\n",
      "Collecting pydeck<1,>=0.8.0b4\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.9/site-packages (from streamlit) (1.4.3)\n",
      "Collecting tenacity<9,>=8.1.0\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.9/site-packages (from streamlit) (3.1.29)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.9/site-packages (from streamlit) (6.2)\n",
      "Collecting rich<14,>=10.14.0\n",
      "  Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.9/site-packages (from streamlit) (9.2.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.9/site-packages (from streamlit) (4.2.4)\n",
      "Collecting altair<6,>=4.0\n",
      "  Using cached altair-5.3.0-py3-none-any.whl (857 kB)\n",
      "Collecting watchdog<5,>=2.1.5\n",
      "  Using cached watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl (83 kB)\n",
      "Collecting toml<2,>=0.10.1\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.9/site-packages (from streamlit) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.9/site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.9/site-packages (from streamlit) (2.27.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
      "Collecting toolz\n",
      "  Using cached toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit) (3.2.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging<25,>=20->streamlit) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas<3,>=1.3.0->streamlit) (2022.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/site-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.27->streamlit) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.27->streamlit) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.27->streamlit) (2.0.12)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/site-packages (from rich<14,>=10.14.0->streamlit) (2.13.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (65.4.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (22.1.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (1.16.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: watchdog, toolz, toml, tenacity, pyarrow, protobuf, mdurl, blinker, pydeck, markdown-it-py, rich, altair, streamlit\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.5\n",
      "    Uninstalling protobuf-3.19.5:\n",
      "      Successfully uninstalled protobuf-3.19.5\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: '/usr/local/etc/jupyter/nbconfig/notebook.d/pydeck.json'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0cb1e04-43ed-4798-b290-77abeab628d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting paramiko\n",
      "  Downloading paramiko-3.4.0-py3-none-any.whl (225 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m225.9/225.9 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.9/site-packages (from paramiko) (38.0.2)\n",
      "Collecting pynacl>=1.5\n",
      "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bcrypt>=3.2\n",
      "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/site-packages (from cryptography>=3.3->paramiko) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko) (2.21)\n",
      "Installing collected packages: bcrypt, pynacl, paramiko\n",
      "Successfully installed bcrypt-4.2.0 paramiko-3.4.0 pynacl-1.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install paramiko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c92dd9-7b56-4ae8-a005-3e5c3db287c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_condition() -> bool:\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from sqlalchemy import create_engine, text\n",
    "    import boto3\n",
    "    import json\n",
    "\n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "\n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "\n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "\n",
    "    # Connect to PostgreSQL\n",
    "    engine = create_engine(f'postgresql+psycopg2://{db_details[\"user\"]}:{db_details[\"password\"]}@{db_details[\"host\"]}:{db_details[\"port\"]}/{db_details[\"dbname\"]}', connect_args={'connect_timeout': 60})\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text('select count(*) from spongebob_outcomes where confirmed != 2;')\n",
    "            data = pd.read_sql_query(query, conn)\n",
    "            count = data.iloc[0]['count']\n",
    "    except Exception as e:\n",
    "        print(\"error\")\n",
    "    \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text('SELECT count(*) FROM metadata_table_spongebob;')\n",
    "            data = pd.read_sql_query(query, conn)\n",
    "            meta_count = data.iloc[0]['count']\n",
    "    except Exception as e:\n",
    "        print(\"error\")\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text('select count(*) from spongebob_outcomes where confirmed != outcome and confirmed !=2;;')\n",
    "            data = pd.read_sql_query(query, conn)\n",
    "            amount_incorrect = data.iloc[0]['count']\n",
    "    except Exception as e:\n",
    "        print(\"error\")\n",
    "\n",
    "    if count >= 10 or amount_incorrect > 2:\n",
    "        try:\n",
    "            with engine.connect() as conn:\n",
    "                delete_query = text(\"DELETE FROM spongebob_outcomes WHERE confirmed != 2;\")\n",
    "                result = conn.execute(delete_query)\n",
    "                conn.commit()\n",
    "        except Exception as e:\n",
    "            print(\"error\")\n",
    "        \n",
    "    if (count >= 4 and count != 0) or meta_count == 0 or amount_incorrect > 2:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c07e27a6-8c5a-4637-a089-140780660129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_condition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0d6d390-8b49-42b4-bf36-c6bbe522422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file():\n",
    "    import boto3\n",
    "    from botocore.exceptions import ClientError\n",
    "    import json \n",
    "    from io import StringIO\n",
    "    import paramiko \n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    import datetime\n",
    "    from sqlalchemy import create_engine\n",
    "    from sqlalchemy import create_engine, Table, Column, Float, Integer, String, MetaData, ARRAY\n",
    "    from sqlalchemy import select, desc, insert, text\n",
    "    \n",
    "    secret_name = \"key\"\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "\n",
    "    secret = get_secret_value_response['SecretString']\n",
    "\n",
    "    # Parse the secret string to get the credentials\n",
    "    secret_dict = json.loads(secret)\n",
    "\n",
    "    original_string = secret_dict['private']\n",
    "    wrapped_key_content = '\\n'.join(original_string[i:i+64] for i in range(0, len(original_string), 64))\n",
    "    formatted_key = f\"-----BEGIN RSA PRIVATE KEY-----\\n{wrapped_key_content}\\n-----END RSA PRIVATE KEY-----\"\n",
    "\n",
    "    bucket_name=\"spongebobpipeline\"\n",
    "    role_arn = 'arn:aws:iam::533267059960:role/aws-s3-access'\n",
    "    session_name = 'kubeflow-pipeline-session'\n",
    "    sts_client = boto3.client('sts')\n",
    "    response = sts_client.assume_role(RoleArn=role_arn, RoleSessionName=session_name)\n",
    "    credentials = response['Credentials']\n",
    "    # Configure AWS SDK with temporary credentials\n",
    "    s3 = boto3.client('s3',\n",
    "                      aws_access_key_id=credentials['AccessKeyId'],\n",
    "                      aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "                      aws_session_token=credentials['SessionToken'])\n",
    "\n",
    "    # SSH connection details\n",
    "    hostname = 'ec2-54-167-24-42.compute-1.amazonaws.com'\n",
    "    port = 22\n",
    "    username = 'ubuntu'\n",
    "\n",
    "    key = formatted_key\n",
    "    key_file = StringIO(key)\n",
    "    # Load the private key\n",
    "    private_key = paramiko.RSAKey.from_private_key(key_file)\n",
    "\n",
    "    # Establish SSH connection\n",
    "    ssh_client = paramiko.SSHClient()\n",
    "    ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh_client.connect(hostname, port, username, pkey=private_key)\n",
    "    \n",
    "    venv = 'yolo/bin/activate'\n",
    "    remote_directory = '/home/ubuntu/objectdetection'\n",
    "    yolo = 'yolo_code'\n",
    "\n",
    "    script_test = 'download.py'\n",
    "\n",
    "    command = f'cd {remote_directory} && source {venv} && cd {yolo} && python {script_test}'\n",
    "    stdin, stdout, stderr = ssh_client.exec_command(command)\n",
    "    #print(stdout.read().decode())\n",
    "    #print(stderr.read().decode())\n",
    "    while True:\n",
    "        # Read from stdout and stderr\n",
    "        line_stdout = stdout.readline()\n",
    "        line_stderr = stderr.readline()\n",
    "\n",
    "        # Print the output\n",
    "        if line_stdout:\n",
    "            print(line_stdout, end='')  # Print stdout line\n",
    "        if line_stderr:\n",
    "            print(line_stderr, end='')  # Print stderr line\n",
    "\n",
    "        # Check if the process is finished\n",
    "        if stdout.channel.exit_status_ready():\n",
    "            break\n",
    "\n",
    "    # Print any remaining output\n",
    "    print(stdout.read().decode())\n",
    "    print(stderr.read().decode())\n",
    "\n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    \n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "\n",
    "    \n",
    "    engine = create_engine(f'postgresql+psycopg2://{db_details[\"user\"]}:{db_details[\"password\"]}@{db_details[\"host\"]}:{db_details[\"port\"]}/{db_details[\"dbname\"]}')\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text('SELECT * FROM metadata_table_spongebob ORDER BY version DESC LIMIT 1;')\n",
    "            data = pd.read_sql_query(query, conn)\n",
    "            version = data['version'].iloc[0] + 1\n",
    "            print(version)\n",
    "    except Exception as e:\n",
    "        version = 1\n",
    "    \n",
    "    typea = {\n",
    "        \"image\": \"bytea\",\n",
    "        \"class\": \"float\",\n",
    "        \"x\": \"float\",\n",
    "        \"y\": \"float\",\n",
    "        \"width\": \"float\",\n",
    "        \"height\": \"float\"\n",
    "    }\n",
    "\n",
    "    typea_json = json.dumps(typea)\n",
    "    \n",
    "    meta_df = pd.DataFrame(data = [[version, datetime.datetime.now(), 6, typea_json]], columns = ['version', 'date', 'features', 'types'])\n",
    "    meta_df.to_sql(\"metadata_table_spongebob\", engine, if_exists='append', index=False)\n",
    "    \n",
    "    # Close the connection\n",
    "    ssh_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b27c0c-62ec-4bf3-8e5a-7001d4c23634",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e003431-1035-4952-a725-16ae464f1a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_op():\n",
    "    import boto3\n",
    "    from botocore.exceptions import ClientError\n",
    "    import json \n",
    "    from io import StringIO\n",
    "    import paramiko \n",
    "    import os \n",
    "    \n",
    "    def get_secret():\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "\n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "    # Retrieve database credentials\n",
    "    (user, pswd, host, port, db) = get_secret()\n",
    "    \n",
    "    secret_name = \"key\"\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "\n",
    "    secret = get_secret_value_response['SecretString']\n",
    "\n",
    "    # Parse the secret string to get the credentials\n",
    "    secret_dict = json.loads(secret)\n",
    "\n",
    "    original_string = secret_dict['private']\n",
    "    wrapped_key_content = '\\n'.join(original_string[i:i+64] for i in range(0, len(original_string), 64))\n",
    "    formatted_key = f\"-----BEGIN RSA PRIVATE KEY-----\\n{wrapped_key_content}\\n-----END RSA PRIVATE KEY-----\"\n",
    "\n",
    "    bucket_name=\"spongebobpipeline\"\n",
    "    role_arn = 'arn:aws:iam::533267059960:role/aws-s3-access'\n",
    "    session_name = 'kubeflow-pipeline-session'\n",
    "    sts_client = boto3.client('sts')\n",
    "    response = sts_client.assume_role(RoleArn=role_arn, RoleSessionName=session_name)\n",
    "    credentials = response['Credentials']\n",
    "    # Configure AWS SDK with temporary credentials\n",
    "    s3 = boto3.client('s3',\n",
    "                      aws_access_key_id=credentials['AccessKeyId'],\n",
    "                      aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "                      aws_session_token=credentials['SessionToken'])\n",
    "\n",
    "    # SSH connection details\n",
    "    hostname = 'ec2-54-167-24-42.compute-1.amazonaws.com'\n",
    "    port = 22\n",
    "    username = 'ubuntu'\n",
    "\n",
    "    key = formatted_key\n",
    "    key_file = StringIO(key)\n",
    "    # Load the private key\n",
    "    private_key = paramiko.RSAKey.from_private_key(key_file)\n",
    "\n",
    "    # Establish SSH connection\n",
    "    ssh_client = paramiko.SSHClient()\n",
    "    ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh_client.connect(hostname, port, username, pkey=private_key)\n",
    "    \n",
    "    venv = 'yolo/bin/activate'\n",
    "    remote_directory = '/home/ubuntu/objectdetection'\n",
    "    yolo = 'yolo_code'\n",
    "\n",
    "    script_to_run = 'yolo_model2.py'\n",
    "\n",
    "    command = f'cd {remote_directory} && source {venv} && cd {yolo} && python {script_to_run}'\n",
    "    stdin, stdout, stderr = ssh_client.exec_command(command)\n",
    "    #print(stdout.read().decode())\n",
    "    #print(stderr.read().decode())\n",
    "    while True:\n",
    "        # Read from stdout and stderr\n",
    "        line_stdout = stdout.readline()\n",
    "        line_stderr = stderr.readline()\n",
    "\n",
    "        # Print the output\n",
    "        if line_stdout:\n",
    "            print(line_stdout, end='')  # Print stdout line\n",
    "        if line_stderr:\n",
    "            print(line_stderr, end='')  # Print stderr line\n",
    "\n",
    "        # Check if the process is finished\n",
    "        if stdout.channel.exit_status_ready():\n",
    "            break\n",
    "\n",
    "    # Print any remaining output\n",
    "    print(stdout.read().decode())\n",
    "    print(stderr.read().decode())\n",
    "\n",
    "    # Close the connection\n",
    "    ssh_client.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5039ef05-acea-4e7a-a78d-5e815c731c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.10.13 torch-2.4.0+cu121 CUDA:0 (NVIDIA A10G, 22516MiB)\n",
      "/home/ubuntu/objectdetection/yolo/lib/python3.10/site-packages/ultralytics/engine/trainer.py:268: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=/home/ubuntu/objectdetection/data.yaml, epochs=20, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=train30, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=0.4, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/home/ubuntu/objectdetection/runs/detect/train30\n",
      "  self.scaler = torch.cuda.amp.GradScaler(enabled=self.amp)\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/ubuntu/objectdetection/yolo_code/combined/train.cache... 728 images, 1 backgrounds, 3 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 729/729 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/ubuntu/objectdetection/yolo_code/combined/test.cache... 225 images, 1 backgrounds, 1 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 226/226 [00:00<?, ?it/s]\n",
      "                   from  n    params  module                                       arguments                     \n",
      "       1/20      2.15G      2.966      6.737       4.17          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:05<00:00,  9.14it/s]\n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 13.78it/s]\n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "       2/20      2.16G      2.971      4.548      3.743          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:03<00:00, 11.92it/s]\n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 17.33it/s]\n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "       3/20      2.16G      2.906      4.075      3.339          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:03<00:00, 12.09it/s]\n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 17.48it/s]\n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "       4/20      2.16G       2.68      3.513      3.237          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:03<00:00, 12.46it/s]\n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.22it/s]\n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "       5/20      2.16G      2.564      3.077      3.096          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:03<00:00, 12.31it/s]\n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 17.65it/s]\n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      "       6/20      2.16G      2.347       2.77       2.98          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:03<00:00, 12.42it/s]\n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 15.05it/s]\n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      "       7/20      2.16G      2.343      2.591      2.949          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:03<00:00, 12.17it/s]\n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 14.67it/s]\n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      "       8/20      2.16G      2.244      2.402      2.883          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:03<00:00, 12.23it/s]\n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.21it/s]\n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      "       9/20      2.16G      2.248      2.325      2.816          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:03<00:00, 12.21it/s]\n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 15.91it/s]\n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      "      10/20      2.16G      2.172      2.181      2.738          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:03<00:00, 12.26it/s]\n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 17.61it/s]\n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "      11/20      2.16G      1.945      2.427      2.662          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:04<00:00, 10.86it/s]\n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 15.34it/s]\n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      "      12/20      2.16G      1.985      2.333      2.715          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:03<00:00, 12.40it/s]\n",
      " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.55it/s]\n",
      "YOLOv8n summary: 225 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
      "      13/20      2.16G      1.921      2.148       2.62          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:03<00:00, 12.39it/s]\n",
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 17.66it/s]\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "      14/20      2.16G      1.911      1.962      2.622          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:03<00:00, 12.48it/s]\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.31it/s]\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "      15/20      2.16G      1.835      1.937      2.556          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:03<00:00, 12.31it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/ubuntu/objectdetection/yolo_code/combined/train/frame_03120_104.00.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0022]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.74it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/ubuntu/objectdetection/yolo_code/combined/train/frame_05820_194.00.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0022]\n",
      "      16/20      2.16G      1.746      1.776      2.505          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:03<00:00, 12.34it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/ubuntu/objectdetection/yolo_code/combined/train/frame_27060_902.00.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0357]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.94it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/ubuntu/objectdetection/yolo_code/combined/test/frame_27060_902.00.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0357]\n",
      "      17/20      2.16G      1.718      1.716       2.48          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:03<00:00, 12.34it/s]\n",
      "Plotting labels to /home/ubuntu/objectdetection/runs/detect/train30/labels.jpg... \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 15.67it/s]\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "      18/20      2.16G      1.764      1.627      2.483          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:03<00:00, 12.41it/s]\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 14.85it/s]\n",
      "Image sizes 640 train, 640 val\n",
      "      19/20      2.16G      1.678      1.552      2.437          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:03<00:00, 12.34it/s]\n",
      "Using 8 dataloader workers\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 15.05it/s]\n",
      "Logging results to \u001b[1m/home/ubuntu/objectdetection/runs/detect/train30\u001b[0m\n",
      "      20/20      2.16G       1.63      1.542      2.339          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:03<00:00, 12.48it/s]\n",
      "Starting training for 20 epochs...\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 15.33it/s]\n",
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  9.58it/s]\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/ubuntu/objectdetection/yolo_code/combined/test.cache... 225 images, 1 backgrounds, 1 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 226/226 [00:00<?, ?it/s]\n",
      "                   all        225        224          0          0          0          0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:01<00:00, 11.93it/s]\n",
      "\n",
      "Traceback (most recent call last):\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "  File \"/home/ubuntu/objectdetection/yolo_code/yolo_model2.py\", line 61, in <module>\n",
      "                   all        225        224          0          0          0          0\n",
      "    'dbname': db,\n",
      "\n",
      "NameError: name 'db' is not defined\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all        225        224          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all        225        224      0.088      0.112     0.0614     0.0249\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all        225        224      0.288     0.0235       0.14     0.0701\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all        225        224      0.253      0.335      0.283      0.109\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all        225        224      0.247      0.235      0.232     0.0752\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all        225        224        0.5      0.118      0.309      0.151\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all        225        224      0.349      0.231      0.289      0.119\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all        225        224      0.444     0.0471      0.247      0.153\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all        225        224      0.362      0.262      0.322      0.157\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all        225        224      0.375     0.0882      0.234      0.111\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all        225        224      0.444     0.0471      0.243       0.13\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all        225        224      0.479      0.135       0.31      0.177\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all        225        224        0.5      0.118      0.309      0.182\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all        225        224        0.5      0.118      0.309      0.169\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all        225        224      0.447      0.347      0.406      0.223\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all        225        224      0.476      0.353      0.417      0.252\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all        225        224      0.395      0.376      0.398      0.235\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all        225        224      0.464      0.382      0.429      0.272\n",
      "\n",
      "20 epochs completed in 0.027 hours.\n",
      "Optimizer stripped from /home/ubuntu/objectdetection/runs/detect/train30/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from /home/ubuntu/objectdetection/runs/detect/train30/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating /home/ubuntu/objectdetection/runs/detect/train30/weights/best.pt...\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.10.13 torch-2.4.0+cu121 CUDA:0 (NVIDIA A10G, 22516MiB)\n",
      "YOLOv8n summary (fused): 168 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "                   all        225        224      0.464      0.382      0.429      0.272\n",
      "             Spongebob         85         85      0.929      0.765      0.857      0.544\n",
      "         Not SpongeBob        139        139          0          0          0          0\n",
      "Speed: 0.1ms preprocess, 1.0ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1m/home/ubuntu/objectdetection/runs/detect/train30\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.10.13 torch-2.4.0+cu121 CUDA:0 (NVIDIA A10G, 22516MiB)\n",
      "YOLOv8n summary (fused): 168 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/ubuntu/objectdetection/yolo_code/combined/test/frame_27060_902.00.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0357]\n",
      "                   all        225        224      0.464      0.382      0.429      0.273\n",
      "             Spongebob         85         85      0.929      0.765      0.857      0.546\n",
      "         Not SpongeBob        139        139          0          0          0          0\n",
      "Speed: 0.1ms preprocess, 3.1ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1m/home/ubuntu/objectdetection/runs/detect/train302\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.10.13 torch-2.4.0+cu121 CPU (AMD EPYC 7R32)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/home/ubuntu/objectdetection/runs/detect/train30/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (5.9 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.4.0+cu121...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success âœ… 1.2s, saved as '/home/ubuntu/objectdetection/runs/detect/train30/weights/best.torchscript' (11.9 MB)\n",
      "\n",
      "Export complete (2.5s)\n",
      "Results saved to \u001b[1m/home/ubuntu/objectdetection/runs/detect/train30/weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/home/ubuntu/objectdetection/runs/detect/train30/weights/best.torchscript imgsz=640  \n",
      "Validate:        yolo val task=detect model=/home/ubuntu/objectdetection/runs/detect/train30/weights/best.torchscript imgsz=640 data=/home/ubuntu/objectdetection/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "model format is /home/ubuntu/objectdetection/runs/detect/train30/weights/best.torchscript\n",
      "Folder './tmp/spongebob/models' already exists.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "911f4cd7-6e2e-4251-8856-1cef945d0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval_deploy() -> None:\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import numpy as np \n",
    "    import json\n",
    "    import os \n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    import boto3\n",
    "    \n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine\n",
    "    \n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1SKLearnSpec\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    \n",
    "    \n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "    \n",
    "    # Connect to PostgreSQL\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_details)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Connected to PostgreSQL successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to PostgreSQL: {e}\")\n",
    "        exit()\n",
    "    \n",
    "    try:\n",
    "        fetch_query = f\"SELECT * FROM spongebob_model_metrics order by map50 desc limit 1;\"\n",
    "        model = pd.read_sql(fetch_query, conn)\n",
    "        accuracy = model['map50'][0]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "    old_version = -1\n",
    "    \n",
    "    try:\n",
    "        fetch_query = \"SELECT * FROM spongebob_model_metrics where in_use is true LIMIT 1;\"\n",
    "        old_model = pd.read_sql(fetch_query, conn)\n",
    "        # old_version = old_model['version'][0]\n",
    "    except Exception as e:\n",
    "        print(f\"a Failed to fetch data: {e}\") \n",
    "    \n",
    "    print(accuracy)\n",
    "    \n",
    "    if old_version == -1 or old_version != model['version'][0]:\n",
    "        print(\"hello\")\n",
    "        if accuracy >= .15:\n",
    "            # Query to fetch data from the table\n",
    "            update_query_new = \"\"\"\n",
    "                UPDATE spongebob_model_metrics\n",
    "                SET in_use = true\n",
    "                WHERE name = %s and version = %s;\n",
    "            \"\"\"\n",
    "\n",
    "            update_query_old = \"\"\"\n",
    "                UPDATE spongebob_model_metrics\n",
    "                SET in_use = false\n",
    "                WHERE name = %s and version = %s;\n",
    "            \"\"\"\n",
    "            try:\n",
    "                cursor.execute(update_query_new, (model['name'][0], int(model['version'][0])))\n",
    "                if(not old_model.empty):\n",
    "                    cursor.execute(update_query_old, (old_model['name'][0], int(old_model['version'][0])))\n",
    "                conn.commit()\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to fetch data: {e}\")\n",
    "        else:\n",
    "            print(\"Bad Accuracy: Email Dovelopers!\")\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c11c022a-a26e-4c76-8c19-bb8fb6fa91b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 14:51:30.170248: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-02 14:51:30.366569: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "2024-08-02 14:51:30.366683: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "2024-08-02 14:51:30.398145: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL successfully.\n",
      "0.43000749587373355\n",
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_eval_deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b8c5127-b695-4dc3-b7ef-fb9ff6a624e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp import components\n",
    "\n",
    "check_condition_op = components.func_to_container_op(func=check_condition, base_image='python:3.7', packages_to_install=['pandas==1.1.5', 'sqlalchemy==1.4.45', 'boto3', 'psycopg2-binary','paramiko'])\n",
    "\n",
    "read_csv_op = components.func_to_container_op(func=read_file, output_component_file='preprocess.yaml', base_image='python:3.7', packages_to_install=['pandas==1.1.5','scikit-learn==1.0.1', 'kfp', 'numpy', 'minio', 'psycopg2-binary', 'sqlalchemy==1.4.45','boto3','paramiko'])\n",
    "\n",
    "train_op = components.func_to_container_op(func=train_op, output_component_file='train.yaml', base_image='python:3.7', packages_to_install=['pandas', 'scikit-learn==1.0.1','numpy','minio', 'tensorflow', 'psycopg2-binary', 'sqlalchemy','boto3','paramiko'])\n",
    "\n",
    "eval_deploy = components.func_to_container_op(func=model_eval_deploy, output_component_file='eval_deploy.yaml', base_image='python:3.7', packages_to_install=['pandas', 'scikit-learn==1.0.1','numpy','minio', 'tensorflow', 'psycopg2-binary', 'sqlalchemy','boto3','kubernetes','kserve','paramiko'])\n",
    "\n",
    "read_data_op = kfp.components.load_component_from_file('preprocess.yaml')\n",
    "train_op = kfp.components.load_component_from_file('train.yaml')\n",
    "eval_deploy_op = kfp.components.load_component_from_file('eval_deploy.yaml')\n",
    "# @dsl.pipeline(\n",
    "#     name='Machine Learning Pipeline',\n",
    "#     description='A pipeline to preprocess, train, and predict using sklearn and tensorflow'\n",
    "# )\n",
    "\n",
    "def ml_pipeline():\n",
    "    check_condition = check_condition_op()\n",
    "    check_condition.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    with dsl.Condition(check_condition.output == 'True'):\n",
    "        preprocess = read_csv_op()\n",
    "        preprocess.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "        train = train_op().after(preprocess)\n",
    "        train.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "        eval_deploy = eval_deploy_op().after(train)\n",
    "        eval_deploy.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "# Compile the pipeline\n",
    "kfp.compiler.Compiler().compile(ml_pipeline, 'spongebob_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd1fd9-08cb-4203-b152-5b9fbae3d8cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
